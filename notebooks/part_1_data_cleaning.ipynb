{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary from EDA and next steps. \n",
    "\n",
    "#### Remove duplicate entries\n",
    "#### Handle missing values\n",
    "#### Filter or correct suspicious URLs\n",
    "#### Normalize and clean titles\n",
    "#### Drop or impute null-like text values\n",
    "#### Standardize and validate categorical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Nat Cat Events.csv')\n",
    "data['seendate'] = pd.to_datetime(data['seendate'], format='%Y%m%dT%H%M%SZ', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be removing duplicated urls and titiles as in real world news dataset it might indicate syndicated content across news outlets, re-posts of the same story, duplicate scraping or ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2176 exact duplicate rows.\n",
      "Removed 0 duplicate URLs.\n",
      "Removed 24144 duplicate titles.\n",
      "Final remaining rows: 65159\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remove full-row duplicates\n",
    "before = data.shape[0]\n",
    "data = data.drop_duplicates()\n",
    "after_full = data.shape[0]\n",
    "print(f\"Removed {before - after_full} exact duplicate rows.\")\n",
    "\n",
    "# Step 2: Remove duplicate URLs (keep first occurrence)\n",
    "before_url = data.shape[0]\n",
    "data = data.drop_duplicates(subset='url')\n",
    "after_url = data.shape[0]\n",
    "print(f\"Removed {before_url - after_url} duplicate URLs.\")\n",
    "\n",
    "# Step 3: Remove duplicate titles (keep first occurrence)\n",
    "before_title = data.shape[0]\n",
    "data = data.drop_duplicates(subset='title')\n",
    "after_title = data.shape[0]\n",
    "print(f\"Removed {before_title - after_title} duplicate titles.\")\n",
    "\n",
    "print(f\"Final remaining rows: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your data\n",
    "domain_parts = data['domain'].apply(extract_domain_parts)\n",
    "data = pd.concat([data, domain_parts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define null-like values\n",
    "null_like = ['none', 'null', 'NaN', 'nan', '', '-', 'n/a', 'unknown']\n",
    "# Columns to clean\n",
    "cat_cols = ['language', 'sourcecountry', 'subdomain', 'domain_root', 'tld']\n",
    "# Replace them with actual np.nan\n",
    "for col in cat_cols:\n",
    "    data[col] = data[col].replace(null_like, np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag and View Suspecious URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the number of suspicious urls  633\n"
     ]
    }
   ],
   "source": [
    "# Flag potentially suspicious URLs by keywords\n",
    "pattern = r'404|notfound|error|invalid|missing'\n",
    "suspicious_mask = data['url'].str.contains(pattern, case=False, na=False)\n",
    "suspicious_urls = data[suspicious_mask]['url']\n",
    "print('this is the number of suspicious urls ', len(suspicious_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accessibility of suspicious URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 633/633 [06:35<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 220 truly inaccessible URLs.\n"
     ]
    }
   ],
   "source": [
    "accessible_mask = suspicious_urls.progress_apply(is_url_accessible)\n",
    "\n",
    "# Filter out inaccessible suspicious URLs\n",
    "urls_to_remove = suspicious_urls[~accessible_mask]\n",
    "print(f\"Removing {len(urls_to_remove)} truly inaccessible URLs.\")\n",
    "data = data[~data['url'].isin(urls_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64939, 11)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty title rows: 1\n"
     ]
    }
   ],
   "source": [
    "# Check for missing titiles\n",
    "empty_titles = data[data['title'].isna() | (data['title'].str.strip() == '')]\n",
    "print(f\"Empty title rows: {len(empty_titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty title rows: 1\n",
      "Attempting to recover titles from URLs using <title> and <h1>/<h2>...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered 0 titles from URLs.\n",
      "1 rows still have empty titles and will be dropped.\n",
      "\n",
      "üîç Sample recovered titles:\n",
      "Series([], Name: url, dtype: object)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify empty titles\n",
    "empty_mask = data['title'].isna() | (data['title'].str.strip() == '')\n",
    "empty_titles = data[empty_mask]\n",
    "print(f\"Empty title rows: {len(empty_titles)}\")\n",
    "\n",
    "# Attempt to recover titles\n",
    "print(\"Attempting to recover titles from URLs using <title> and <h1>/<h2>...\")\n",
    "recovered_titles = empty_titles['url'].progress_apply(get_best_title_from_url)\n",
    "data.loc[empty_titles.index, 'title'] = recovered_titles\n",
    "\n",
    "# Count what was recovered\n",
    "recovered_count = recovered_titles.notna().sum()\n",
    "still_empty_count = recovered_titles.isna().sum()\n",
    "print(f\"Recovered {recovered_count} titles from URLs.\")\n",
    "print(f\"{still_empty_count} rows still have empty titles and will be dropped.\")\n",
    "\n",
    "# Drop rows still missing titles\n",
    "data = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-ASCII/Corrupted Titles Found: 3095\n",
      "58                Magnitude 4 . 3 earthquake jolts¬†Nepal\n",
      "78     Warnings mount after quakes shake T√ºrkiye sout...\n",
      "103    Japan issues tsunami alert after series of str...\n",
      "138    Japan earthquake ‚Äì live : Major tsunami warnin...\n",
      "203    Major tsunami hits Japan coast amid magnitude¬†...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Corrupted / Non-English Titles \n",
    "non_ascii_mask = ~data['title'].apply(lambda x: isinstance(x, str) and x.isascii())\n",
    "print(\"Non-ASCII/Corrupted Titles Found:\", non_ascii_mask.sum())\n",
    "print(data[non_ascii_mask]['title'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These titles are relevant and valid, but they were likely flagged as ‚Äúcorrupted‚Äù because they contain non-ASCII characters, such as:\n",
    "\n",
    "T√ºrkiye ‚Üí contains a special √º\n",
    "\n",
    "‚Äì ‚Üí is an en dash, not a simple hyphen -\n",
    "\n",
    "‚Äô ‚Üí a curly quote instead of '\n",
    "\n",
    "‚Ä¶ or em dashes ‚Äî, special accents, etc.\n",
    "Rather than removing these rows, you can normalize or clean these characters:\n",
    "\n",
    "I will be using Unicode normalization with unicodedata\n",
    "This converts curly quotes, accented letters, en dashes, etc. to ASCII where possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalisation to the title (replacing non english characters)\n",
    "data['normalized_title'] = data['title'].apply(normalize_unicode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-ASCII/Corrupted Titles Found: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any Corrupted / Non-English Titles left\n",
    "non_ascii_mask = ~data['normalized_title'].apply(lambda x: isinstance(x, str) and x.isascii())\n",
    "print(\"Non-ASCII/Corrupted Titles Found:\", non_ascii_mask.sum())\n",
    "# so now all titles are normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_title'] = data['normalized_title'].apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64939/64939 [14:37<00:00, 74.02it/s] \n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER) for Location using Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # lightweight but effective for GPE/LOC\n",
    "\n",
    "def extract_location(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "    return locations if locations else None\n",
    "\n",
    "data['locations'] = data['clean_title'].progress_apply(extract_location)\n",
    "data['has_location'] = data['locations'].apply(lambda x: bool(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Extracting nat_cat events from titles useing exact keyword match\n",
    "natcat_keywords = [\n",
    "    'earthquake', 'flood', 'storm', 'tornado', 'hurricane', \n",
    "    'landslide', 'tsunami', 'volcano', 'wildfire', 'mudslide', 'eruption'\n",
    "]\n",
    "\n",
    "def natcat_keyword_info(text):\n",
    "    text_lower = text.lower()\n",
    "    for keyword in natcat_keywords:\n",
    "        if keyword in text_lower:\n",
    "            return pd.Series([True, keyword])\n",
    "    return pd.Series([False, None])\n",
    "\n",
    "data[['has_natcat_keyword', 'matched_keyword']] = data['clean_title'].apply(natcat_keyword_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based matches: 20599\n",
      "Rows needing smart classification: 44340\n"
     ]
    }
   ],
   "source": [
    "# Count how many rows satisfy BOTH conditions\n",
    "rule_based_matches = data[data['has_location'] & data['has_natcat_keyword']]\n",
    "print(f\"Rule-based matches: {len(rule_based_matches)}\")\n",
    "\n",
    "# Create a mask for rows NOT matching both\n",
    "not_matched_mask = ~(data['has_location'] & data['has_natcat_keyword'])\n",
    "to_classify = data[not_matched_mask]\n",
    "print(f\"Rows needing smart classification: {len(to_classify)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Will be saving that spacy_and_exact_match processed data for further usage \n",
    "rule_based_matches.to_csv('rule_based_nat_cat_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of this notebook:\n",
    "## Data Cleaning & Feature Engineering for Natural Disaster Event Detection\n",
    "This pipeline focuses on preparing and enriching news/event data to support downstream prediction of natural disaster-related content.\n",
    " 1. Data Deduplication\n",
    "Removed full-row duplicates, then dropped repeated url and title entries.\n",
    "\n",
    "Reduced redundancy and ensured each row represents a unique event.\n",
    "\n",
    "2. Domain Parsing & Categorical Cleaning\n",
    "Extracted subdomain, domain_root, and tld from domain using tldextract.\n",
    "\n",
    "Standardized null-like values (e.g., 'none', 'n/a') across categorical features like language and sourcecountry.\n",
    "\n",
    "3. URL Quality Checks\n",
    "Flagged and removed suspicious or broken URLs based on keywords (404, error, etc.) and HTTP accessibility checks.\n",
    "\n",
    "4. Title Normalization & Recovery\n",
    "Recovered missing titles using header and title, etc. from webpage content (requests, BeautifulSoup).\n",
    "\n",
    "Removed non-ASCII characters and normalized text to produce clean_title.\n",
    "\n",
    "5. Location Extraction (NER)\n",
    "Used spaCy to extract geographic entities (e.g., cities, countries) from clean_title.\n",
    "\n",
    "Added binary has_location and locations columns.\n",
    "\n",
    "6. Natural Disaster Keyword Matching\n",
    "Identified mentions of events like earthquake, flood, wildfire, etc.\n",
    "\n",
    "Created features: has_natcat_keyword and matched_keyword.\n",
    "\n",
    "Outcome & Next Steps\n",
    "Rule-based matches (with both location and disaster keyword): 20,599 rows\n",
    "\n",
    "Remaining for ML classification: 44,340 rows\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
